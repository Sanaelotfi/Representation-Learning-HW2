# IFT6135: Representation Learning, second assignement.

## Description

This is the second assignement of the [IFT6135: Representation-Learning](https://sites.google.com/mila.quebec/ift6135), taught by Prof. [Aaron Courville](https://mila.quebec/en/person/aaron-courville/).

## Theory

Optimization, Regularization, and Recurrent Neural networks (RNNs) : [Statement and Solution](https://github.com/Sanaelotfi/Representation-Learning-HW2/blob/master/IFT6135_HW2_Theory.pdf).


## Practice

Recurrent Neural Networks, Optimization, and Attention : [Statement](https://github.com/Sanaelotfi/Representation-Learning-HW2/blob/master/HW2_practice_statement.pdf).

This is a joint work with Abderrahim Khalifa, Yann Bouteiller and Amine Bellahsen. 

### Problem 1: implementing a simple RNN

[Solution](https://github.com/Sanaelotfi/Representation-Learning-HW2/blob/master/Problem1-2-3.ipynb).


### Problem 2: implementing an RNN with Gated Recurrent Units (GRU)

[Solution](https://github.com/Sanaelotfi/Representation-Learning-HW2/blob/master/Problem1-2-3.ipynb).

### Problem 3: implementing the attention module of a transformer network 

[Solution](https://github.com/Sanaelotfi/Representation-Learning-HW2/blob/master/Problem1-2-3.ipynb).

### Problem 4: training language models

[Solution](https://github.com/Sanaelotfi/Representation-Learning-HW2/blob/master/Problem4.ipynb).

### Problem 5: detailed evaluation of trained models

[Solution](https://github.com/Sanaelotfi/Representation-Learning-HW2/blob/master/Problem%205.ipynb).
